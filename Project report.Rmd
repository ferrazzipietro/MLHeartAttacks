---
title: "ML project"
author: "Pietro & Marita"
datae: "2/24/2022"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/ML/MLHeartAttacks")
```

```{r,  include = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
library(gridExtra)
library(GGally)
library(ggthemes) # Load
library(ggpubr)
library(ggplot2)
library(DataExplorer)

library(survival)
library(survminer)
library(MASS)
library(skimr)
library(corrplot)
library(tidyverse)
library(mlr)
library(mlrCPO)

setwd("~/Desktop/ML/MLHeartAttacks")
#setwd("C:/Users/marit/OneDrive - Universit√© Libre de Bruxelles/Bureau/BDMA/UPC/ML/PROJECT")
data <- read.csv('Heart failure.csv', sep=";") #heart....
data.table <- data
data <- data %>% as_tibble()
n <- nrow(data)
```

Two aricles where these data come from:

1.  Chicco, D., Jurman, G. Machine learning can predict survival of
    patients with heart failure from serum creatinine and ejection
    fraction alone. BMC Med Inform Decis Mak 20, 16 (2020).
    [DOI](https://doi.org/10.1186/s12911-020-1023-5)

2.  Ahmad T, Munir A, Bhatti SH, Aftab M, Raza MA (2017) Survival
    analysis of heart failure patients: A case study. PLoS ONE 12(7):
    e0181001. [DOI](https://doi.org/10.1371/journal.pone.0181001)

These are the data we have about the survavial from

```{r, include=FALSE}
data
skim(data)
```

*Gender*, *Smoking*, *Diabetes*, *BP* and *Anaemia* are factors that
have been saved as numerical values. To start, we decided to transform
them into categorical variables

```{r echo=FALSE}
data <- data%>% mutate(Event=as.factor(data$Event),
                     Gender=as.factor(Gender),
                     Smoking=as.factor(Smoking),
                     Diabetes=as.factor(Diabetes),
                     BP=as.factor(BP),
                     Anaemia=as.factor(Anaemia))
```

# DESCRIPTION OF THE DATA

... IMPORTANT: explain difference btw usual data with only
classification of the event and this problem, that requires survival
models.

Before starting looking at the data we randomly split it in one set for
training and one for testing. We are splitting this two sets of data
with an 80-20 ratio. We want to use as much data as possible for
training since we have few instances, while being able to get a good
estimation of the best model at the end of this analysis. We also make
this split stratified, which means keeping the proportion of positive
cases in the subsamples as it appears in the full data set. We want our
models to learn the data as it is in reality, quite unbalanced.
Splitting data in train and test will assure that the observations we
will do during the explorative analysis and the decisions that will be
made on them will not be biased by the knowledge acquired from the test
set. What follows is based on the training set.

```{r, echo=FALSE, warning = FALSE}
desc_outer <- makeResampleDesc("Holdout", split = 3/4, stratify = TRUE)
final_task <- makeClassifTask(id = "HeartFailure", data = data, target = "Event", positive = 1)

set.seed(1234)
partition_outer <- makeResampleInstance(desc_outer, final_task) 

train.idxs <- partition_outer$train.inds[[1]]
test.idxs <- partition_outer$test.inds[[1]]
tot <- data
test <- tot[-train.idxs,]
train <- tot[train.idxs,]
data <- as.data.frame(train)

# necessary for survival models
tot.table <- data.table
test.table <- tot.table[-train.idxs,]
train.table <- tot.table[train.idxs,]
data.table <- train.table
```

# EXPLORATIVE ANALYSIS

We started with the explorative analysis of the data. The aim of this
first section is to reach a general understanding of the data we have,
their distribution, the correlation between variables and in general all
the aspects that concern the descriptive analysis of the observations.
Only after a rigorous analysis we can start to model the data, since
before that we will not have a complete comprehension of variables and
their relationships.

## Pairs of continuous variables

First, we are interested to see the joint and disjoint distribution of
all the pairs of continous variables.

```{r graphs1, echo=FALSE}
g <- ggplot(data, aes(y=TIME))
mycolors <- c("Corr"="deepskyblue")
ggpairs(data %>% dplyr::select(Age, Ejection.Fraction, Sodium, 
                       Creatinine,Pletelets, CPK ),
        aes(col=rep("Corr",224),
            fill=rep("Corr",224),
            alpha = 0.4))  +
  scale_fill_manual(values = mycolors, 
    aesthetics = c("colour", "fill")) 

```

The correlation is significantly different from zero only for the
couples *Creatinine-Age*, *Creatinine-Sodium* and
*Sodium-Ejection.Fraction*, and anyways very low.

## Factors

The, we plotted the binomial variables in order to see if their
distribution is balanced.

```{r, echo = FALSE}
g <- ggplot(data,aes(col=rep("Corr",224), fill=rep("Corr",224)))
plot_bar(data,
         ncol    = 3,
         title   = "Observations per value",
         ggtheme = theme_bw(),
         theme_config = list(plot.title = element_text(size = 16, face = "bold"),
                             strip.text = element_text(colour = 1, size = 10, face = 2),
                             legend.position = "none"))

```

These distribution plots show that the proportion of statistical units
with *Event*=1 is only `r sum(data$Event==1)/n`. Even *Gender*,
*Smoking* and *BP* have unbalanced distributions, but the number of
elements in the two levels for all these variables are enough to say
that we are not in presence of rare classes.

In the BMC article cited above, the conclusion is drawn that the
variables *Ejection.Fraction* and *Creatinine* are the best performers
when training the models. Therefore, we want to put special emphasis on
the correlation between these two predictors and the response variable
*Event*. Since these are numerical and the response variable is
continuous, we must perform a correlation test according to these
characteristics: Point-Biserial Correlation.

```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(ltm)
cat("Biserial Correlation between Ejection.Fraction and Event is: ", biserial.cor(train$Ejection.Fraction, train$Event))
cat("Biserial Correlation between Ejection.Fraction and Event is: ", biserial.cor(train$Creatinine, train$Event))
```

The fact that the two best predictors according to BMC article have a
correlation with the response variable of approximately 0.3 will make
our predictions rather mediocre. This will be a clear limitation in the
project, because not only we have few instances available but now we
encountered low correlations.

## Time and factors

An important aspect to consider is how the time under observation is
related to the other variables. The following boxplots show that there
is not relation between the time and the binomial variables.

```{r graphs2, echo=FALSE, fig.height = 4, fig.width = 8, fig.align = "center"}
g <- ggplot(data, aes(y=TIME))


g2 <- g + geom_boxplot(aes(x=as_factor(Gender), fill=Gender)) +
  labs(title="",x="", y = "", color="Gender")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g3 <- g + geom_boxplot(aes(x=as_factor(Smoking), fill=Smoking)) +
  labs(title="",x="", y = "", color="Smoking")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g4 <- g + geom_boxplot(aes(x=as_factor(Diabetes), fill=Diabetes)) +
  labs(title="",x="", y = "", color="Diabetes")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g5 <- g + geom_boxplot(aes(x=as_factor(BP), fill=BP)) +
  labs(title="",x="", y = "", color="BP")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g6 <- g + geom_boxplot(aes(x=as_factor(Anaemia), fill=Anaemia)) +
  labs(title="",x="", y = "", color="Anaemia")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()

ggarrange(ggarrange(g2,g3,g4, ncol=3, nrow=1) ,ggarrange(g5,g6), ncol = 1, nrow = 2, align="h", widths = c(0.05, 0.05, 0.5)) %>% 
          annotate_figure(top=text_grob("TIME",face="bold"))
```

This result is interesting, since it tell us that the experiment from
which the data has been collected is unbiased in respect of differences
between the persons under observation. In other words, the selected
people were kept under observation independently from the collected
variables that described themselves. It is important because it means
that we can rely the way data has been collected and use it to provide
results that will not be biased.

We have to consider aside the box plot about *TIME* in respect to the
*Event*. It is clear that the time under observation for people that
have presented an hart attack is lower than the the one of the ones that
did not. As expected, this indicates that the heart attack looks to
reduce the time under observation. The opposite would have been
unexpected. It can seems obvious, but not having this result would have
lead us to conclude that the data were not able to explain the relation
between the explicative variables and the answer variable.

```{r graph3, fig.height = 2, fig.width = 3, fig.align = "center", echo=FALSE }

g1 <- g + geom_boxplot(aes(x=as.factor(Event), fill=Event),
                       varwidth=FALSE) +
  labs(title="",x="Event", y = "TIME", color="Event") +
  scale_fill_brewer(palette="Blues")  +
  theme_minimal()
g1 + theme(axis.text=element_text(size=8),
        axis.title=element_text(size=9),
        legend.title =element_text(size=9) ,
        legend.text =element_text(size=9))
```

## Continuous variables vs Event or categorical variables

Then, we looked at the relation between the pairs of categorical and
continuous variables, included the *Event.*

The scope of the following analysis is double:

a)  to understand if the event of having an heart attack is related to
    different distributions of the continuous variables, i.e. if the
    population of people having *Event*=1 is the same of the one having
    *Event*=0

b)  to understand if and how the categorical variables are related to
    the continuous ones.

## Age vs factors

```{r graphs4, echo=FALSE, fig.height = 4, fig.width = 8}
g <- ggplot(data, aes(y=Age))

g1 <- g + geom_boxplot(aes(x=as.factor(Event), fill=Event)) +
  labs(title="",x="", y = "", color="Event")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()

g2 <- g + geom_boxplot(aes(x=as_factor(Gender), fill=Gender)) +
  labs(title="",x="", y = "", color="Gender")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g3 <- g + geom_boxplot(aes(x=as_factor(Smoking), fill=Smoking)) +
  labs(title="",x="", y = "", color="Smoking")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g4 <- g + geom_boxplot(aes(x=as_factor(Diabetes), fill=Diabetes)) +
  labs(title="",x="", y = "", color="Diabetes")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g5 <- g + geom_boxplot(aes(x=as_factor(BP), fill=BP)) +
  labs(title="",x="", y = "", color="BP")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g6 <- g + geom_boxplot(aes(x=as_factor(Anaemia), fill=Anaemia)) +
  labs(title="",x="", y = "", color="Anaemia")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
ggarrange(g1,g2,g3,g4,g5,g6,
          ncol = 3, nrow = 2) %>% annotate_figure(top=text_grob("Age", face="bold"))
```

It seems that the population on which the event has occurred is older
than the one that did not observed it. It is reasonable, since heart
attack are generally more frequent in older people. For all the other
variables, there is basically no difference between the groups.

Some points are more than 1.5 times the interquartile distance far from
the median, but that are few and there is not enough evidence to
consider them as outliers. Anyways, it is something that we should keep
in mind.

As expected, the youngest observed age is forty.

## Ejection Fraction vs factors

```{r graphs5, echo=FALSE, fig.height = 4, fig.width = 8}
g <- ggplot(data, aes(y=Ejection.Fraction))
g1 <- g + geom_boxplot(aes(x=as_factor(Event), fill=Event)) +
  labs(title="",x="", y = "", color="Event")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()

g2 <- g + geom_boxplot(aes(x=as_factor(Gender), fill=Gender)) +
  labs(title="",x="", y = "", color="Gender")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g3 <- g + geom_boxplot(aes(x=as_factor(Smoking), fill=Smoking)) +
  labs(title="",x="", y = "", color="Smoking")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g4 <- g + geom_boxplot(aes(x=as_factor(Diabetes), fill=Diabetes)) +
  labs(title="",x="", y = "", color="Diabetes")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g5 <- g + geom_boxplot(aes(x=as_factor(BP), fill=BP)) +
  labs(title="",x="", y = "", color="BP")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g6 <- g + geom_boxplot(aes(x=as_factor(Anaemia), fill=Anaemia)) +
  labs(title="",x="", y = "", color="Anaemia")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
ggarrange(g1,g2,g3,g4,g5,g6,
          ncol = 3, nrow = 2) %>% annotate_figure(top=text_grob("Ejection.Fraction",
                                                                face="bold"))
```

It seems that the population on which the event has occurred has lower
values of *Ejection fraction*. That means that this variable can be
useful to describe the distribution of the vent over the population.

About the relation between this variable and the categorical ones, we
can say something similar to what we said for the *Age*.

For men, the *Ejection fraction* looks to be lower as for persons with
*Diabetes*. The remaining ones are not really related to this continuous
variable.

## Sodium vs factors

```{r graphs6, echo=FALSE, fig.height = 4, fig.width = 8}
g <- ggplot(data, aes(y=Sodium))
g1 <- g + geom_boxplot(aes(x=as.factor(Event), fill=as.factor(Event))) +
  labs(title="",x="", y = "", color="Event")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()

g2 <- g + geom_boxplot(aes(x=as_factor(Gender), fill=as.factor(Gender))) +
  labs(title="",x="", y = "", color="Gender")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g3 <- g + geom_boxplot(aes(x=as_factor(Smoking), fill=as.factor(Smoking))) +
  labs(title="",x="", y = "", color="Smoking")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g4 <- g + geom_boxplot(aes(x=as_factor(Diabetes), fill=as.factor(Diabetes))) +
  labs(title="",x="", y = "", color="Diabetes")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g5 <- g + geom_boxplot(aes(x=as_factor(BP), fill=as.factor(BP))) +
  labs(title="",x="", y = "", color="BP")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g6 <- g + geom_boxplot(aes(x=as_factor(Anaemia), fill=as.factor(Anaemia))) +
  labs(title="",x="", y = "", color="Anaemia")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
ggarrange(g1,g2,g3,g4,g5,g6, ncol = 3, nrow = 2) %>% 
          annotate_figure(top=text_grob("Sodium", face="bold"))
```

In this situation data look to be less regular. There are multiple
points that are enough far from the median to allow us to say that a
deeper analysis of outliers could be considered. In particular, some
data present an unusual low value of *Sodium*.

Talking about differences between the distribution of the amount of
*Sodium* in the two groups, there is not difference to be noticed. On
the other hand, this variable does not look very useful in term of
explaining the distribution of the *Event*, since the level of *Sodium*
is almost the same for both the groups of people having had an heart
attack while being under observation and people who didn't.

## Pletelets vs factors

```{r graphs7, echo=FALSE, fig.height = 4, fig.width = 8}
g <- ggplot(data, aes(y=Pletelets))
g1 <- g + geom_boxplot(aes(x=as.factor(Event), fill=as.factor(Event))) +
  labs(title="",x="", y = "", color="Event")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()

g2 <- g + geom_boxplot(aes(x=as_factor(Gender), fill=as.factor(Gender))) +
  labs(title="",x="", y = "", color="Gender")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g3 <- g + geom_boxplot(aes(x=as_factor(Smoking), fill=as.factor(Smoking))) +
  labs(title="",x="", y = "", color="Smoking")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g4 <- g + geom_boxplot(aes(x=as_factor(Diabetes), fill=as.factor(Diabetes))) +
  labs(title="",x="", y = "", color="Diabetes")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g5 <- g + geom_boxplot(aes(x=as_factor(BP), fill=as.factor(BP))) +
  labs(title="",x="", y = "", color="BP")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g6 <- g + geom_boxplot(aes(x=as_factor(Anaemia), fill=as.factor(Anaemia))) +
  labs(title="",x="", y = "", color="Anaemia")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
ggarrange(g1,g2,g3,g4,g5,g6, ncol = 3, nrow = 2) %>% 
          annotate_figure(top=text_grob("Pletelets", face="bold"))
```

The *Pletelets'* value has not distribution changes between the people
having had the heart attack and the others. This suggest that this
variable is not really useful for our scopes.

Furthermore, we can notice that there is not difference in distribution
between this continuous variable and all the others.

## Cratininine vs factors

At the beginning of our analysis we noticed that its distribution has a
long right tail. In order to provide a sensefull visualization, we
removed those very high value. More specifically, we removed from the
visualization observation with a level of Creatinine higher than 5.

#ADD LEGEND TO THE GRAPH

```{r graphs8, fig.height = 1.5, fig.width = 5, fig.align = "center", echo=FALSE, warning=FALSE}
data_for_creatinine <- data %>% dplyr::select(Creatinine)
idxLessThan5 <- data_for_creatinine$Creatinine<5
LessThan5 <- data_for_creatinine$Creatinine
LessThan5[! idxLessThan5]=NA
data_for_creatinine <- data_for_creatinine %>% add_column( LessThan5 = LessThan5)

ggplot(data_for_creatinine) + stat_density(aes(Creatinine), adjust = 2, alpha=1, col="lightblue",geom="line") +
  theme_minimal() + labs(y="Density") +
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=13,face="bold"),
        legend.title =element_text(size=13,face="bold") ,
        legend.text =element_text(size=12,face="bold")) +
 stat_density(aes(data_for_creatinine$LessThan5), adjust = 2, alpha=1, fill=2, col="navy",geom="line") 
```

```{r graphs9, echo=FALSE, fig.height = 4, fig.width = 8}
g <- ggplot(data %>% filter(Creatinine<5), aes(y=Creatinine))
g1 <- g + geom_boxplot(aes(x=as.factor(Event), fill=as.factor(Event)), coef=2.5) +
  labs(title="",x="", y = "", color="Event")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()

g2 <- g + geom_boxplot(aes(x=as_factor(Gender), fill=as.factor(Gender)), coef=2.5) +
  labs(title="",x="", y = "", color="Gender")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g3 <- g + geom_boxplot(aes(x=as_factor(Smoking), fill=as.factor(Smoking)), coef=2.5) +
  labs(title="",x="", y = "", color="Smoking")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g4 <- g + geom_boxplot(aes(x=as_factor(Diabetes), fill=as.factor(Diabetes)), coef=2.5) +
  labs(title="",x="", y = "", color="Diabetes")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g5 <- g + geom_boxplot(aes(x=as_factor(BP), fill=as.factor(BP)), coef=2.5) +
  labs(title="",x="", y = "", color="BP")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g6 <- g + geom_boxplot(aes(x=as_factor(Anaemia), fill=as.factor(Anaemia)), coef=2.5) +
  labs(title="",x="", y = "", color="Anaemia")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
ggarrange(g1,g2,g3,g4,g5,g6,
          ncol = 3, nrow = 2) %>% annotate_figure(top=text_grob("Creatinine",
                                                                face="bold"))
```

As saw before, the distribution of the *Creatinine* variable is uneven.
Anyways, it seems that this variable can be considered between the ones
useful to forecast the event, since people having observed the event
during the observation time seems to have higher value of *Creatinine*.

On the other hand, the other categorical variables do not present
differences in distribution in the two groups identify by the two levels
of each categorical variable.

## CPK vs factors

Before analyzing the level of enzyme *CPK*, we had to apply the same
process we applyed to the *Creatinine* variable. In fact, its
distribution is unbalanced:

```{r graphs10, fig.height = 1.5, fig.width = 5, fig.align = "center", echo=FALSE}

ggplot(data) + stat_density(aes(CPK), adjust = 2,
                           alpha=0.7, col="navy", fill="lightblue") +
  theme_minimal() + labs(y="Density") +
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=13,face="bold"),
        legend.title =element_text(size=13,face="bold") ,
        legend.text =element_text(size=12,face="bold"))
```

This curve is very similar to the distribution of the *Creatinine*
variable. This relation between the two of them can also been seen
throgh the following graphs:

```{r graphs11, echo=FALSE, fig.height = 4, fig.width = 8}
g <- ggplot(data %>% filter(CPK<4000), aes(y=CPK))
g1 <- g + geom_boxplot(aes(x=as_factor(Event), fill=Event)) +
  labs(title="",x="", y = "", color="Event")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()

g2 <- g + geom_boxplot(aes(x=as_factor(Gender), fill=Gender)) +
  labs(title="",x="", y = "", color="Gender")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g3 <- g + geom_boxplot(aes(x=as_factor(Smoking), fill=Smoking)) +
  labs(title="",x="", y = "", color="Smoking")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g4 <- g + geom_boxplot(aes(x=as_factor(Diabetes), fill=Diabetes)) +
  labs(title="",x="", y = "", color="Diabetes")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g5 <- g + geom_boxplot(aes(x=as_factor(BP), fill=BP)) +
  labs(title="",x="", y = "", color="BP")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
g6 <- g + geom_boxplot(aes(x=as_factor(Anaemia), fill=Anaemia)) +
  labs(title="",x="", y = "", color="Anaemia")  +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()

ggarrange(g1,g2,g3,g4,g5,g6,
          ncol = 3, nrow = 2) %>% annotate_figure(top=text_grob("CPK", face="bold"))
```

Even the distribution of CPK is unbalanced to the rigth, with a lot of
big values. Unfortunately, the *CPK*/*Event* boxplot shows that the
former variable is probably not really useful to explain the latter one.
The relation between the level of CPK and the other factors is the same
of the *Creatinine* variable.

From this latter graphs, we can conclude that the relation between these
two variables ( *Creatinine* and *CPK*) is very strong. The difference
between the two of them is only in the effect that they have on the
distribution of the *Event*. While the *Creatinine* level looks to have
an effect on the heart attacks, the *CPK* does not.

We should take into account this observation while performing variable
selection.

## Pairs of continuous variables and Event

The next step is to look for patterns between pairs of continuous
variables and the fact that the event was observed (or not).

```{r graphs12, fig.align = "center", echo=FALSE, fig.height = 12, fig.width=15}
col <- scale_color_gradient(high=1, low = 0, 
                            n.breaks=4,
                            breaks = waiver())

g <- ggplot(data, aes(col=Event))
g1 <- g + geom_point(aes(x=Sodium, y=Age),lwd=1)
g2 <- g + geom_point(aes(x=Sodium,  y=Creatinine),lwd=1) 
g3 <- g + geom_point(aes(x=Sodium, y=CPK),lwd=1) 
g4 <- g + geom_point(aes(x=Sodium, y=Pletelets),lwd=1) 
g5 <- g + geom_point(aes(x=Sodium, y=Ejection.Fraction),lwd=1) 
g6 <- g + geom_point(aes(x=CPK, y=Age),lwd=1) 
g7 <- g + geom_point(aes(x=CPK, y=Creatinine),lwd=1) 
g8 <- g + geom_point(aes(x=CPK, y=Pletelets),lwd=1) 
g9 <- g + geom_point(aes(x=CPK, y=Ejection.Fraction),lwd=1) 
g10 <- g + geom_point(aes(x=Pletelets, y=Creatinine),lwd=1) 
g11 <- g + geom_point(aes(x=Pletelets, y=Age),lwd=1) 
g12 <- g + geom_point(aes(x=Pletelets, y=Ejection.Fraction),lwd=1) 
g13 <- g + geom_point(aes(x=Age, y=CPK), lwd=1)
g14 <- g + geom_point(aes(x=Age, y=Ejection.Fraction), lwd=1)
g15 <- g + geom_point(aes(x=CPK, y=Ejection.Fraction), lwd=1)

ggarrange(g1,g2,g3,g4,g5,g6,g7,g8,g9,g10,g11,g12,g13,g14,g15,
          ncol = 3, nrow = 5, common.legend = TRUE) %>% 
  annotate_figure(top=text_grob("Distribution of events over pairs of var", face="bold"))
```

There are not natural clusters defined by these composition of
variables.

## Conclusions of the explorative analysis

To summarize, we have seen that:

-   there is a strong relation between variables *CPK* and *Creatinine*;

-   the level of *Creatinine* looks to have an high impact in the
    *Event*;

-   the level of *Sodium*, the *Age* and the *Ejection Fraction* look to
    have a low impact on the Event variable;

-   the number of *Pletelets* in the blood and the level of *CPK* have
    no impact on the Event variable.

-   there are not natural clusters defined by pairs of continuous
    variables

-   the categorical variables do not seem to influence the *Event*.

All these aspect are fundamental to have a first overview of the
information contained in the data. It does not mean that these will
certainly be the variable that will be used in the classification
models, but it will help us in better understanding the nature of the
case-study.

# SURVIVAL ANALYSIS

As said in the introduction to the data, this dataset has a specificity
regarding the way we the variable *Event* has been collected. More
precisely, each patient had been under observation for a certain number
of days (variable *TIME*). That means that the *Event* could be observed
only under the period of observation of each statistical unit. This
considerations leaded us to use the traditional approach for data of
this type, i.e. survival analysis. Our main idea is that this kind of
approach will better explain the behavior of the data, leading us to a
deeper comprehension of the relation between the variables and the
*Event* and, most of all, to a better choice of the features to include
in our classification models.

## Non-parametric analysis

In the survival context, the first thing to do is to look at the
survival curves using the Kaplain-Meyer method (more details at Goel,
Manish Kumar et al. "Understanding survival analysis: Kaplan-Meier
estimate." International journal of Ayurveda research vol. 1,4 (2010):
274-8. <doi:10.4103/0974-7788.76794>).

### Individual effect of the factors

First, we evaluated the dichotomous variables.

```{r surv1, echo=FALSE, fig.height = 4}
s1 <- Surv(data$TIME, data$Event)
surv_m1 <- survfit(s1~data$Gender, error="greenwood",
                   conf.type="log", se.fit=TRUE, conf.int=0.95,
                   data=data)

par(mfrow=c(3,2))
g <- list()
Evento <- as.numeric(levels(as.factor(data$Event)))[as.factor(data$Event)]
data$Evento=Evento
g[[1]] <- ggsurvplot(survfit(Surv(TIME, (Evento))~Gender, data=data),
                 conf.int=T, pval.method=T, log.rank.weights="1", data=data, palette = c("navy","lightblue"), legend="none") + labs(x="Time", y="Survival for Gender")
g[[2]] <- ggsurvplot(survfit(Surv(TIME, (Evento))~Smoking, data=data),
                 conf.int=T, pval.method=T, log.rank.weights="1", data=data, palette = c("navy","lightblue"), legend="none") + labs(x="Time", y="Survival for Smoking")
g[[3]] <- ggsurvplot(survfit(Surv(TIME, (Evento))~Diabetes, data=data),
                 conf.int=T, pval.method=T, log.rank.weights="1", data=data, palette = c("navy","lightblue"), legend="none") + labs(x="Time", y="Survival for Diabetes")
g[[4]] <- ggsurvplot(survfit(Surv(TIME, (Evento))~BP, data=data),
                 conf.int=T, pval.method=T, log.rank.weights="1", data=data, palette = c("navy","lightblue"), legend="none") + labs(x="Time", y="Survival for BP")
g[[5]] <- ggsurvplot(survfit(Surv(TIME, (Evento))~Anaemia, data=data),
                 conf.int=T, pval.method=T, log.rank.weights="1", data=data, palette = c("navy","lightblue"), legend="none") + labs(x="Time", y="Survival for Anaemia")
arrange_ggsurvplots(g, print = TRUE,
  ncol = 3, nrow = 2, risk.table.height = 0.4)
# ggarrange(g1,g2,g3,g4,g5,
#           ncol = 2, nrow = 3) %>% 
#   annotate_figure(top=text_grob("Distribution of events over pairs of var",
#                                 face="bold"))
```

Each pair of survival curves defined by different levels of the
categorical variables have confidence intervals that overlaps, as it can
be seen throw the fact that the red area and the light blue one always
overlap. This means that, from a descriptive point of view, these
variables do not have effects on the answer. This is in line with what
we saw in the previous explorative analysis.

### Joint effect of the factors

```{r surv test, echo=FALSE}
ss <- survdiff(Surv(TIME, Evento)~Gender+Smoking+Diabetes+BP+Anaemia, data=data, rho=0)
# ggsurvplot(surv_fit(Surv(TIME, Evento)~Gender+Smoking+Diabetes+BP+Anaemia, data=data))
df <- nrow(ss$var)-1
```

We can perform a general test to see if the combination of all these
factors, taken together, has significant effect on the survival curve.

Previously, we saw the comparison between survival curves of groups of
people individuated by each of the factors, taken individually. From
those graphs, we could notice that the lines generally does not
intersect. On this base, we can reasonably use a *logarithmic rank test*
for the hypothesis that all the survival curves defined by all the
combination of the possible values of the binomial variables are equal.
The overall p-value of this test is
`r pchisq(ss$chisq, df, lower.tail=F)`. That means that, even if
individually the variables does not look to be very useful, *their
combination can be used to explain the behavior of the heart attacks
among the population of interest* .

To conclude, we can say that each of the categorical variable does not
seem to have impact on the event if individually considered, the
combination of all the categorical variables seems to have impact on the
event.

These results imply that, while doing variables selection and, in
general, building our models, we will have to pay attention in finding
the set of variables which the joints effect is explicative, even if the
individual ones are not.

# Survival models

We can now apply survival regression models to better understand the
utility of the different variables in explaining the survival ratio. In
other words, we will make some assumption over the data, apply a
parametric model that can be build on the top of those assumptions,
check the bounty of this model and, if the measures are indicating that
it well fit the data, extract conclusion from its output.

It should be noticed that we are still working in a descriptive
paradigm. Our scope is not to provide previsions on future observations,
but only to understand, using the training set, which are the useful
variables. We applied a Weibull model.

*WEIBULL MODEL*

Assumption:

-   the residuals follows a Weibull distribution \[wikipedia
    reference\](<https://en.wikipedia.org/wiki/Weibull_distribution>)

-   the risks of observing the event for different populations are
    proportional

-   the hazard ratio is constant (i.e., the risk for a person with
    certain levels of the explicative variables divided by the risk for
    a person with different levels of those variables is constant)

Model: $S(time)=e^{-\lambda time^ \alpha}$

First, we trained the model with all the variables. The optimal starting
model would be the one with all the possible interactions since we
noticed in the previous non parametric analysis that some variables are
ineffective if individually considered but could be important if jointly
taken into account. Unfortunately, the small amount of data we have
prevent us to apply the complete model. We will then manually adapt some
forward selection to individuate the useful features and discard the
others. The selection is semiautomatic based on the Akaike Information
Criteria.

```{r, echo=FALSE, center=TRUE}
exponential_survreg <- survreg(Surv(TIME,Event)~Gender+Smoking+Diabetes+BP+
                                 Anaemia+Age+Ejection.Fraction+Creatinine+Pletelets,
                               data=data.table, dist="weibull")
summary(exponential_survreg)$table %>% round(2)
```

The variables *Gender*, *Smoking*, \_Diabetes_and *Pletelets* are not
significant for this model. Before discard them we tried, for each of
them, to see if their interactions with the other variables are to be
kept in the model.

```{r, echo=FALSE}
exponential_survreg <- survreg(Surv(TIME,Event)~
                                 Smoking:Gender+
                                 Smoking:Diabetes+
                                 Smoking:Pletelets+
                                 Smoking:BP+
                                 Smoking:Anaemia+
                                 Smoking:Age+
                                 Smoking:Ejection.Fraction+
                                 Smoking:Creatinine+
                                 Gender+Diabetes+Pletelets+BP+Anaemia+Age+Ejection.Fraction+Creatinine,
                               data=data.table, dist="weibull")
final <- stepAIC(exponential_survreg, trace=0 )
s <- summary(final)
# s$table %>% round(2)
```

The only significant ones are *Smoking:Gender* and *Smoking:Age*.

```{r, echo=FALSE}
exponential_survreg <- survreg(Surv(TIME,Event)~
                                 Smoking:Gender+
                                 Smoking:Age+
                                 Pletelets:Gender+
                                 Pletelets:Diabetes+
                                 Pletelets:Pletelets+
                                 Pletelets:BP+
                                 Pletelets:Anaemia+
                                 Pletelets:Age+
                                 Pletelets:Ejection.Fraction+
                                 Pletelets:Creatinine+
                                 Diabetes+Pletelets+BP+Anaemia+Age+Ejection.Fraction+Creatinine,
                               data=data.table, dist="weibull")
final <- stepAIC(exponential_survreg, trace=0)
s <- summary(final)
#s$table %>% round(2)
```

This process has been applied for all the variables, obtaining the
following model:

```{r, echo=FALSE}
exponential_survreg <- survreg(Surv(TIME,Event)~
                                 Smoking:Gender+
                                 Smoking:Age+
                                 Pletelets:Gender+
                                 Pletelets:Diabetes+
                                 Pletelets:Pletelets+
                                 Pletelets:BP+
                                 Pletelets:Anaemia+
                                 Pletelets:Age+
                                 Pletelets:Ejection.Fraction+
                                 Pletelets:Creatinine+
                                 Diabetes+Pletelets+BP+Anaemia+Age+Ejection.Fraction+Creatinine+Gender,
                               data=data.table, dist="weibull")
final <- stepAIC(exponential_survreg, trace=0)
s <- summary(final)
# s$table %>% round(2)

exponential_survreg <- survreg(Surv(TIME,Event)~
                                 Smoking:Gender+
                                 Smoking:Age+
                                 Pletelets:Gender+
                                 Anaemia:Diabetes+
                                 Anaemia:BP+
                                 Anaemia:Ejection.Fraction+
                                 Anaemia:Creatinine+
                                 Diabetes+Pletelets+BP+Anaemia+Age+Ejection.Fraction+Creatinine+Gender,
                               data=data.table, dist="weibull")
final <- stepAIC(exponential_survreg, trace=0)
s <- summary(final)
# s$table %>% round(2)
# drop Anaemia

exponential_survreg <- survreg(Surv(TIME,Event)~
                                 Smoking:Gender+
                                 Smoking:Age+
                                 Pletelets:Gender+
                                 Age:Diabetes+
                                 Age:BP+
                                 Age:Ejection.Fraction+
                                 Age:Creatinine+
                                 Pletelets+Age+Diabetes+BP+Ejection.Fraction+Creatinine+Gender,
                               data=data.table, dist="weibull")
final <- stepAIC(exponential_survreg, trace=0)
s <- summary(final)
# s$table %>% round(2)

exponential_survreg <- survreg(Surv(TIME,Event)~
                                 Smoking:Gender+
                                 Smoking:Age+
                                 Pletelets:Gender+
                                 Age:Ejection.Fraction+
                                 Diabetes:BP+
                                 Diabetes:Creatinine+
                                 Diabetes:Ejection.Fraction+
                                 Pletelets+Age+Diabetes+BP+Ejection.Fraction+Creatinine+Gender,
                               data=data.table, dist="weibull")
final <- stepAIC(exponential_survreg, trace=0)
s <- summary(final)
# s$table %>% round(2)

exponential_survreg <- survreg(Surv(TIME,Event)~
                                 Smoking:Gender+
                                 Smoking:Age+
                                 Pletelets:Gender+
                                 Age:Ejection.Fraction+
                                 Diabetes:Ejection.Fraction+
                                 BP:Creatinine+
                                 BP:Ejection.Fraction+
                                 Pletelets+Age+Diabetes+BP+Ejection.Fraction+Creatinine+Gender,
                               data=data.table, dist="weibull")
final <- stepAIC(exponential_survreg, trace=0)
s <- summary(final)
# s$table %>% round(2)

exponential_survreg <- survreg(Surv(TIME,Event)~
                                 Smoking:Gender+
                                 Smoking:Age+
                                 Pletelets:Gender+
                                 Age:Ejection.Fraction+
                                 Diabetes:Ejection.Fraction+
                                 Creatinine:Ejection.Fraction+
                                 Pletelets+Age+Diabetes+BP+Ejection.Fraction+Creatinine+Gender,
                               data=data.table, dist="weibull")
final <- stepAIC(exponential_survreg, trace=0)
s <- summary(final)
# s$table %>% round(2)

# final model
final <- survreg(Surv(TIME,Event)~
                                 Smoking:Gender+
                                 Smoking:Age+
                                 Pletelets:Gender+
                                 Age:Ejection.Fraction+
                                 Diabetes:Ejection.Fraction+
                                 Creatinine:Ejection.Fraction+
                                 Pletelets+Age+Diabetes+BP+Ejection.Fraction+Gender,
                               data=data.table, dist="weibull")
# summary(final)$table %>% row.names()
```

It has to be noticed that the scale parameter is not significantly far
from 0. It means that the (easier) exponential model would be a better
choice for this data. Based on this observation, we then decided to use
it: $S(time)=e^{-\lambda time}$. The results in terms of variables to be
kept are the same.

We then have the following model:

```{r, echo=FALSE}
# final model
final <- survreg(Surv(TIME,Event)~
                                 Smoking:Gender+
                                 Smoking:Age+
                                 Pletelets:Gender+
                                 Age:Ejection.Fraction+
                                 Diabetes:Ejection.Fraction+
                                 Creatinine:Ejection.Fraction+
                                 Pletelets+Age+Diabetes+BP+Ejection.Fraction+Gender,
                               data=data.table, dist="weibull", scale=1)
s <- summary(final)
s$table[,c(1,4)] %>% round(2)
```

Usual methodologies to asses the bounty of models are not appropriate
for these family of models. In fact, usually there is no the concept of
time under observation to be taken into account. This exponential model,
on the other hand, deal with that time. For this reason, we performed
the assessment of the fit of the final model using the *Cox-Snell
residuals*. The approach allowed us to verify about correctness of the
distribution assumptions. The idea is that the relation between these
residuals and the logaritm of the cumulative hazard ratio estimated
using the Fleming-Harrington method is linear. We then plotted these two
variables obtaining a confirm about this linear relation. It means the
the assumption of exponential distribution is quite fine.

```{r, echo=FALSE}
y_pred= predict(final, na.action = "na.omit", type="linear")
rS <- ( log(data.table$TIME) - y_pred ) # res standardizzati 
rSo = sort(unique(rS))
Sres = survfit(Surv(rS,data.table$Event)~1, ctype=2 ) # "fleming-harrington"
plot(rSo,log(-log(Sres$surv)), xlim=c(-7,1),
     xlab="Cox-Snell residuals", 
     ylab="Log of Cumulative Lazard",
     main="Bounty of the Final Survival Model",
     col = "navy",
     pch=19,
     cex=0.7)
abline(0,1, lwd=3, col="lightblue")
```

The process through which we selected these variables was, as already
mentioned, purely based on the AIC. It leaded to a model were there are
iterations of terms that are not individually present. In order to
increment the interpretability of the results, we decided to add those
variables anyways.

# CHECK IF MY IDEAS MAKE SENSE

### Meaning of these interactions

-   *Smoking* and *Gender*: the data is collected in Pakistan. It could
    be reasonable to think that there are not collected variables that
    could distinguish women that smoke from the ones that does not. For
    example, it could be that women that smoke are from specific social
    subgroups where other risky behaviors are more frequent than in the
    average men population (e.g., the women that smoke are western women
    with different habits from Pakistani people).

-   *Gender* and *Pletelets*: see Gender-based differences in platelet
    function and platelet reactivity to P2Y12 inhibitors Ranucci M,
    Aloisio T, Di Dedda U, Menicanti L, de Vincentiis C, et al. (2019)
    Gender-based differences in platelet function and platelet
    reactivity to P2Y12 inhibitors. PLOS ONE 14(11): e0225771.
    [DOI](https://doi.org/10.1371/journal.pone.0225771)

-   *Age* and *Ejection Fraction*: the increment of the ejection
    fraction leads to a decrease of the survival ratio (of
    `r final$coef[11]`) for each more year of age. This result in
    completely aligned with experimental results (Chuang, Michael L et
    al. "Association of age with left ventricular volumes, ejection
    fraction and concentricity: the Framingham heart study." Journal of
    Cardiovascular Magnetic Resonance vol. 15,Suppl 1 P264. 30 Jan.
    2013, [DOI](https://doi.org/10.1186/1532-429X-15-S1-P264).

-   *Ejection Fraction* and *Diabetes*: the increment of the ejection
    fraction leads to a decrease of the survival ratio (of
    `r final$coef[12]`) for people with diabetes. This is out of the
    range of our knoweldge and after some research we were able to say
    that this is still an open issue (Ehl NF, K√ºhne M, Brinkert M,
    M√ºller-Brand J, Zellweger MJ. Diabetes reduces left ventricular
    ejection fraction--irrespective of presence and extent of coronary
    artery disease. Eur J Endocrinol. 2011 Dec;165(6):945-51.
    [doi](https://doi.org/10.1530/EJE-11-0687) Epub 2011 Sep 8. PMID:
    21903896.)

-   *Ejection Fraction* and *Creatinine*: the same as before.

## CPK and Creatinine

1)  In the previous analysis we saw that *CPK* and *Creatinine* are
    highly related. On the other hand, the correlation between the two
    is very low: \$Cor\_{Creat,CPK} =\\\\\$
    `r round(cor(data$CPK, data$Creatinine), 3)`.

2)  The explorative analysis showed that *CPK* doesn't seem useful to
    classify the *Event*.

3)  In addition, we just saw that it is not even founded to be used
    neither using the Kaplain-Meyer methor nor the Exponential models.

4)  Furthermore, we applied a generalized linear model with only *CPK*
    and *Creatinine* to explain the *Event*.

```{r, include=FALSE, echo=FALSE}
m1 <- glm(Event ~ CPK + Creatinine, data=data, family=binomial, )
summary(m1)
m2 <- glm(Event ~ CPK + I(CPK^2) + I(CPK^3) , data=data, family=binomial)
summary(m2)

r1 <- as.matrix(round(summary(m1)$coef[2:3,4],2))
colnames(r1) <- "Pval"
r2 <- as.matrix(round(summary(m2)$coef[2:4,4],2))
colnames(r2) <- "Pval"
```

The scope was to answer the question if both variables were useful if
jointly used to (linearly) classify the Event. To look at this kind of
linear dependence we have generated two models:

-   $logit(Event) = \beta_0 + \beta_1 CPK + \beta_2 Creatinine + \epsilon \\$
-   $logit(Event) = \beta_0 + \beta_1 CPK + \epsilon \\$

The significances of the parameters of the first model are,
respectively: `r r1`.

Before definitely discharging CPK as a variable for the classification
of the level of Event, we performed two other tests to see even over the
linear dependency. In order to do that, we built the following model:
$logit(Event) = \beta_0 + \beta_1 CPK + \beta_2 CPK^2 + \beta_3 CPK^3 + \epsilon \\$.
All these coefficients where not significantly different from 0. In
fact, their p-values are respectively: `r r2`.

On the basis of these 5 different approaches the gave all the same
result we decided to drop *CPK* can be dropped for our dataset. In our
context, it means that CPK is not considered useful in inferring the
*Event* of interest if combined with *Creatinine.*

### Conclusions

In conclusion, it can be said that the variables that significantly
directly impact the mortality curves are *Pletelets*, *Age*, *Diabetes*,
*BP*, *Ejection.Fraction* and *Gender*. *Smoking1* and *Creatinine* have
impact only if combined with some of the others. Relying of these
results, we can create some new features on the top of the original
ones. More precisely, we are generating new features representing the
iterations that were founded as significant by the Exponential model.

```{r, echo=FALSE}
data_surv_tot <- tot %>%
  dplyr::select(c(-TIME, -CPK, -Anaemia, Pletelets, Age, Diabetes, BP, Ejection.Fraction, 
                  Gender, Smoking, Creatinine)) %>% 
  add_column( Smoking_Gender = (tot$Smoking %>% as.numeric)  * (tot$Gender%>% as.numeric) ) %>%
  add_column( Smoking_Age = (tot$Smoking%>% as.numeric) * tot$Age) %>%
  add_column( Gender_Pletelets = (tot$Gender%>% as.numeric) * tot$Pletelets) %>%
  add_column( Age_Ejection.Fraction = tot$Age * tot$Ejection.Fraction) %>%
  add_column( Ejection.Fraction_Diabetes = tot$Ejection.Fraction * (tot$Diabetes%>% as.numeric)) %>%
  add_column( Ejection.Fraction_Creatinine = tot$Ejection.Fraction * tot$Creatinine) 
```

# CLASSIFICATION

# MLR package

We will be using R package *mlr* as a framework. It follows the
following pattern:

1.  Define task: what will be predicted and any consideration given the
    data to be used (e.g. partitioning)
2.  Create learner: which ML algorithm will be used, as well as
    hyperparameters, preprocessing, etc.
3.  Determine resample and measure: which validation strategy will be
    followed and which metric will be used.
4.  Train the model
5.  Predict and obtain model accuracy ( in our case, we will use the true positive rate)

In our case, since there are many models that we want to apply, we will
be doing a benchmark before actually training. This will give us a
glimpse of the accuracies that these methods would get. After that, we
will be tuning hyperparameters in the best performing models.

## Creating the task and Validation partitioning

K-Fold-Cross-Validation

If the sample size is small, it is recommended to use repeated
k-Fold-Cross-Validation, as it achieves a good bias-variance balance
and, given that there are not many observations, the computational cost
is not excessive. (In other words, you could even do a high number of
folds because it will take little time). For instance, in our benchmark
we will use 5 folds with 10 reps, which is not equivalent to
50-folds-cross-validation. The issue with partitioning data with few
instances is that results may depend on luck. That's why we are
considering 10 different partitions with 5 folds, so results will not
depend on chance, but the average will be closer to reality.

# Machine Learning Analysis

CHANGE THIS !!!

Why mlrCPO
[Reference](https://www.rdocumentation.org/packages/mlrCPO/versions/0.3.4/topics/mlrCPO-package)

Basically you define a pipeline using the mlrCPO pipeop %>\>%. Every
pipeop you put before the learner will be applied directly before the
training but after the train test split.

## Classification Analysis

We will not consider the variable *Time* from now on because it would
not be information that we would obtain from a new patient, it is only a
control variable.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
data_classif <- as.data.frame(data) %>% dplyr::select(-TIME) %>%
                mutate_at(vars(Gender, Smoking, Diabetes, BP, Anaemia), as.numeric) 

classif_task <- makeClassifTask(id = "HeartFailure", data = data_classif, target = "Event", positive = 1) # data only contains training data

set.seed(1234)
desc_inner <- makeResampleInstance("RepCV", reps = 5,  folds = 2, task = classif_task, stratify = TRUE)
```

```{r, warning = FALSE, echo=FALSE, message=FALSE}
# https://mlr.mlr-org.com/articles/tutorial/integrated_learners.html

classif_measures <- list(tpr, timetrain)
classif_pipeline <- cpoScaleRange() 

## GLM ##
library(glmnet)
classif_lrn_glm <- makeLearner("classif.glmnet", id = "glm", predict.type = "prob")
classif_lrn_glm <- classif_pipeline %>>% classif_lrn_glm

## Naive Bayes ##
classif_lrn_naive <- makeLearner("classif.naiveBayes", id = "naive", predict.type = "prob")
classif_lrn_naive <- classif_pipeline %>>% classif_lrn_naive

## KNN Model ##
library(kknn)
classif_lrn_knn <- makeLearner("classif.kknn", id = "knn", predict.type = "prob")
classif_lrn_knn <- classif_pipeline %>>% classif_lrn_knn

## LDA ##
library(MASS)
classif_lrn_lda <- makeLearner("classif.lda", id = "LDA", predict.type = "prob")
classif_lrn_lda <- classif_pipeline %>>% classif_lrn_lda

## QDA ##
classif_lrn_qda <- makeLearner("classif.qda", id = "QDA", predict.type = "prob")
classif_lrn_qda <- classif_pipeline %>>% classif_lrn_qda

## Tree(rpart) ##
classif_lrn_rpart <- makeLearner("classif.rpart", id = "rpart", predict.type = "prob")
classif_lrn_rpart <- classif_pipeline %>>% classif_lrn_rpart

## Random Forest ##
classif_lrn_rf <- makeLearner("classif.randomForest", id = "rf", predict.type = "prob")
classif_lrn_rf <- classif_pipeline %>>% classif_lrn_rf

## Ranger ##
classif_lrn_ranger <- makeLearner("classif.ranger", id = "ranger", predict.type = "prob")
classif_lrn_ranger <- classif_pipeline %>>% classif_lrn_ranger

## SVM ##
classif_lrn_SVM <- makeLearner("classif.svm", id = "SVM", predict.type = "prob")
classif_lrn_SVM <- classif_pipeline %>>% classif_lrn_SVM


## Neural Net ##
# library(neuralnet)
classif_lrn_neuralnet <- makeLearner("classif.neuralnet", id = "neuralnet", predict.type = "prob")
classif_lrn_neuralnet <- classif_pipeline %>>% classif_lrn_neuralnet

## Extreme Gradient Boosting ##
classif_lrn_xgboost <- makeLearner("classif.xgboost", id = "xgboost", eval_metric = "aucpr", predict.type = "prob")
classif_lrn_xgboost <- classif_pipeline %>>% classif_lrn_xgboost

learners <- list(classif_lrn_glm, classif_lrn_naive, classif_lrn_knn, classif_lrn_rpart, 
                 classif_lrn_rf, classif_lrn_ranger, classif_lrn_SVM, 
                 classif_lrn_neuralnet, classif_lrn_xgboost)
# classif_lrn_ada,
library(parallelMap)
parallelStartSocket(4) # start in socket mode and create 2 processes on localhost

set.seed(1234)
bmr_classif <- benchmark(learners    = learners, 
                         tasks       = classif_task, 
                         resamplings = desc_inner, 
                         measures    = classif_measures, 
                         models      = TRUE,
                         show.info   = TRUE)
bmr_classif

parallelStop()
```

### Filter Selection

```{r, echo=FALSE, warning=FALSE, message=FALSE}
data_fs <- data %>% dplyr::select(-TIME)

fs_task <- makeClassifTask(id = "HeartFailure", data = data_fs, target = "Event", positive = 1)
```

In order to decide which features would be selected to see if they help
the models get better predictions, we will perform filter selection.
Unlike feature selection, this method does not require of a learner to
reach a conclusion. We have checked that depending on the learner
selected, results change drastically. Filter selection was found to be a
more impartial methodology.

The metric used for this filter selection is *Information Gain*. It
measures the reduction in entropy (or surprise) by splitting a dataset
according to a given value of a random variable. A larger information
gain suggests a lower entropy group or groups of samples, and hence less
surprise.
[Reference](https://machinelearningmastery.com/information-gain-and-mutual-information/)

Entropy quantifies how much information there is in a random variable,
or more specifically its probability distribution. A skewed distribution
has a low entropy, whereas a distribution where events have equal
probability has a larger entropy.

```{r, echo=FALSE, messagge=FALSE}
library(FSelectorRcpp)
fv <- generateFilterValuesData(fs_task, method = "FSelectorRcpp_information.gain")
plotFilterValues(fv, feat.type.cols = TRUE) + scale_fill_brewer(palette="Blues") + ggpubr::theme_pubr() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  ggtitle("Filter Selection \n Information Gain")

fv$data[order(-value)]
```

We saw in the BMC article that one of their conclusions was: "Our
results of these two-feature models show not only that *serum creatinine* and *ejection fraction* are sufficient to predict survival
of heart failure patients from medical records, but also that using
these two features alone can lead to more accurate predictions than
using the original data set features in its entirety."

Therefore, considering the results obtained, the conclusions of the
previous analysis, and that the correlation between the response
variable and Sodium is approximately 0.2, we have decided to use only
*serum creatinine* and *ejection fraction* for the comparison of
results.

```{r, echo=FALSE, messagge=FALSE}
data_fs <- data %>% dplyr::select(Creatinine, Ejection.Fraction, Event)

fs_task <- makeClassifTask(id = "HeartFailure", data = data_fs, target = "Event", positive = 1)

set.seed(1234)
desc_inner_fs <- makeResampleInstance("RepCV", reps = 5,  folds = 2, task = fs_task, stratify = TRUE)
```

```{r, warning = FALSE, echo=FALSE, message=FALSE}
# https://mlr.mlr-org.com/articles/tutorial/integrated_learners.html

# Using the same learners defined in previous chunk

parallelStartSocket(4) # start in socket mode and create 2 processes on localhost

set.seed(1234)
bmr_fs <- benchmark(learners    = learners, 
                    tasks       = fs_task, 
                    resamplings = desc_inner_fs, 
                    measures    = classif_measures, 
                    models      = TRUE,
                    show.info   = FALSE)
bmr_fs

parallelStop()
```

```{r, message=FALSE, fig.height=4, fig.width=10, echo=FALSE}
bp1 <- plotBMRBoxplots(bmr_classif, 
                       measure   = tpr, 
                       order.lrn = getBMRLearnerIds(bmr_classif))  +
       aes(fill = learner.id)  +
       labs(x = "", y = "True Positive Rate") + scale_fill_brewer(palette="Blues")+
  ylim(0,0.8)

bp3 <- plotBMRBoxplots(bmr_fs, 
                       measure   = tpr, 
                       order.lrn = getBMRLearnerIds(bmr_classif))  +
       aes(fill = learner.id)  +
       labs(x = "", y = "True Positive Rate (FSel)") + scale_fill_brewer(palette="Blues") +
  ylim(0,0.8)

ggarrange(bp1, bp3, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
```

Here, our aim is to maximize *TPR*, because in the medical area it is
better to have false alarms (overestimating positives) than not
predicting real positives. Some models may have a good accuracy (even
better than other models), but that is not the priority in healthcare
analysis (such as predicting cancer and other diseases). For instance, a
trivial model (e.g. predicting all patients as class 0), would give very
high accuracies, not taking into account any of the data provided.
That's why we will decide which model is better based on *TPR* (true
positive rate).

In this stage, we would like to decide whether we use the whole dataset
or the subset obtained selecting some features. From the results we can
comment on three main points:

-   Weak learners perform visibly worse, some of them even reaching a
    tpr of around 0.2.

-   K-nearest neighbor performs surprisingly better when selecting few
    features. This can be explained because the algorithm does not work
    well with high dimensions. Performance changed from 0.3 to 0.6, with
    a default of 7 neighbors. Support Vector Machine also benefits from
    less features.

-   Lastly, we see that overall models perform slightly better with the
    feature selection. However, results vary much more depending on the
    partition. In view of the results, we decide to perform tuning for
    both datasets using a subset of the best performing learners. We
    will be tuning mainly "strong learners" (i.e. excluding *glm*,
    *naive bayes*) as well as *SVM*, because we believe they will be
    able to discard useless features and reach better performances while
    being consistent, not fluctuating much.

For now, we should mention that *rpart* (tree) is accomplishing good
results, and would be convenient for this project since it is easy to
explain.

### Tuning models

In this section, we are going to tune different hyperparameters for the
following models: kknn, rpart, random forest, ranger, extreme gradient
boosting, and neural network. Below we will explain the reasoning behind
selecting each parameter. The same validation method as for the previous
analysis will be used, i.e. Repeated Cross Validation with 2 folds and 5
iterations. For the hyperparameter optimization, random search will be
executed. It is shown to be more efficient in a paper by Bergstra, J.
and Bengio, Y.
(2012).[Reference](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf).
A total of 200 iterations will be splitted among the 7 learners chosen
for tuning.

```{r, message = FALSE, echo=FALSE}
bls <- list(makeLearner("classif.kknn"),
            makeLearner("classif.rpart"), 
            makeLearner("classif.randomForest"),
            makeLearner("classif.ranger"),
            makeLearner("classif.xgboost", eval_metric = "aucpr"),
            makeLearner("classif.neuralnet"))

lrn <- makeModelMultiplexer(bls)

# Parameter Space (all together, divided by learner inside)
ps <- makeModelMultiplexerParamSet(
  multiplexer = lrn,
  classif.kknn = makeParamSet(makeIntegerParam("k", lower = 5, upper = 15)), # Default is 7-
  classif.rpart = makeParamSet(makeIntegerParam("minsplit", lower = 15, upper = 35), # Default is 20
                               makeIntegerParam("maxdepth", lower = 15, upper = 50)), # Default is 30
  classif.randomForest = makeParamSet(makeIntegerParam("ntree", lower = 20, upper = 500), #Default is 500
                                      makeIntegerParam("mtry",lower = 1, upper = 2), #Default is floor(sy,qrt(features/3)), cannot be larger than number of variables
                                      makeIntegerParam("nodesize", lower = 1, upper = 200)), #Default is 1
  classif.ranger = makeParamSet(makeIntegerParam("mtry", lower = 1, upper = 2), # Same parameters as randomForest
                                makeIntegerParam("num.trees", lower = 1, upper = 200),
                                makeIntegerParam("min.node.size", lower = 10, upper = 50)),
  classif.xgboost = makeParamSet(makeNumericParam("eta", lower = 0.2, upper = 0.5), # Default is 0.3
                                 makeIntegerParam("nrounds", lower = 4, upper = 10), # Default is 
                                 makeIntegerParam("max_depth", lower = 3, upper = 10)), # Default is 6
  classif.neuralnet = makeParamSet(makeIntegerVectorParam("hidden", len = 1, lower = 1, upper = 10),
                                   makeNumericParam("threshold", lower = 2, upper = 5, trafo = function(x) {10^x}),
                                   makeNumericParam("stepmax", lower = 2, upper = 3, trafo = function(x) {10^x}))
)

# Random Search
control_grid <- makeTuneControlRandom(maxit = 500L) # Default is 100, split between the amount of learners


parallelStartSocket(5) # start in socket mode and create 2 processes on localhost

set.seed(1234)
tuning_results_all <- tuneParams(learner = lrn, 
                                 task = classif_task, 
                                 resampling = desc_inner, 
                                 par.set = ps, 
                                 control = control_grid, 
                                 measures = list(tpr),
                                 show.info = TRUE)

set.seed(1234)
tuning_results_fs <- tuneParams(learner = lrn, 
                                task = fs_task, 
                                resampling = desc_inner, 
                                par.set = ps, 
                                control = control_grid, 
                                measures = list(tpr),
                                show.info = TRUE)
parallelStop()
```

# THIS GRAPH DOES NOT MAKE SENSE. DROP IT
```{r, fig.height=4, fig.width=12, echo=FALSE}
#Check results
res_iters_all <- getTuneResultOptPath(tuning_results_all)
t_all <- res_iters_all %>% dplyr::select(selected.learner, tpr.test.mean)

p_all <- ggplot(t_all, aes(tpr.test.mean, selected.learner)) + 
         geom_boxplot(aes(fill = selected.learner))  + 
         labs(x = "True Positive Rate", y = "") + scale_fill_brewer(palette="Blues")


res_iters_fs <- getTuneResultOptPath(tuning_results_fs)
t_fs <- res_iters_fs %>% dplyr::select(selected.learner, tpr.test.mean)

p_fs <- ggplot(t_fs, aes(tpr.test.mean, selected.learner)) + 
         geom_boxplot(aes(fill = selected.learner))  + 
         labs(x = "True Positive Rate (FSel)", y = "") + scale_fill_brewer(palette="Blues")


ggarrange(p_all, p_fs, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
```
The best model (in terms of true positive rate) is a Neural Network with ......
hidden layers and ....


Finally, based on these graphs we can conclude that selecting few
features is essential for this study. Excluding *neuralnet*, that
clearly fails for this data set, all models are consistent in the
right-side graph. The two bagging models (*ranger* and *randomForest*)
get equivalent results, so one of these two should be used in the final
model.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Obtaining best iteration for each learner (selecting features)
best_res <- res_iters_fs %>% group_by(selected.learner) %>% slice(which.max(tpr.test.mean)) %>%
            relocate(tpr.test.mean, .after = selected.learner)
library(reshape)
options(scipen=999)
temp <- melt(as.data.frame(best_res)) %>% dplyr::select(-error.message) %>% drop_na() 

tpr.test <- temp %>% filter(grepl("tpr", variable)) %>% dplyr::rename(tpr = value) %>% dplyr::select(-variable)
hyperparam <- temp %>% filter(grepl("classif", variable)) 

tuning_res <- full_join(hyperparam, tpr.test) %>% 
              mutate(parameter = ifelse(grepl("\\.", variable), sub(".*\\.(.*)", "\\1", variable), "")) %>%
              dplyr::select(selected.learner, tpr, parameter, value) 

rm(temp, tpr.test, hyperparam)
tuning_res
```

#### Explanation of parameters

-   RPART For the model *rpart* we are tuning two of the "stopping"
    parameters in the algorithm, that tells the tree when to stop
    growing (a way of pruning)

    -   *minsplit*: minimum number of observations that must exist in a
        node in order for a split to be attempted. If it is too small,
        the tree will keep growing and probably lead to overfitting, if
        it is too big the accuracy may decrease.
    -   *maxdepth*: Set the maximum depth of any node of the final tree,
        with the root node counted as depth 0. Again, if we let the tree
        make too many splits, it will lead to overfitting. A tree that
        is too small may generalize too much.

-   RANDOM FOREST

    -   *ntree*: this should not be set to too small a number, to ensure
        that every input row gets predicted at least a few times.
    -   *mtry*: number of variables to possibly split at in each node.
        (Cannot be bigger than the number of variables).
    -   *nodesize*: Setting this number larger causes smaller trees to
        be grown (and thus preventing from overfitting and takes less
        time)

-   RANGER Same parameters as in random forest, since it's the same
    algorithm, just optimized.

-   SVM

    -   *cost*: is the penalty parameter, which represents
        misclassification or error term. If it's smaller, the decision
        boundary will have a big margin, resulting in more
        missclassified data. If it's bigger, the penalty is higher and
        the algorithm tries to minimize missclassifications.
    -   *gamma*: large gamma values will probably lead to overfitting.
        Small values will generalize too much.

-   ADA

    -   *nu*: This parameter is provided to shrink the contribution of
        each classifier, sometimes called learning rate. Reducing this
        parameter will mean the weights will be increased or decreased
        to a small degree, forcing the model train slower, although
        sometimes results in better performance scores.
    -   *iter*: number of boosting iterations to perform (total number
        of weak learners).

-   XGBOOST

    -   *eta*: control the learning rate. Used to prevent overfitting by
        making the boosting process more conservative.
    -   *nrounds*: max number of boosting iterations.
    -   *max_depth*: maximum depth of a tree

-   NEURALNET

    -   *hidden*: hidden neurons, for each layer (only one layer in our
        case since it will make it faster obtaining very good results)
        To help the algorithm converge when enlarging *hidden*, we need
        to make the *threshold* and *stepmax* bigger as well.
    -   *threshold*: threshold for the partial derivatives of the error
        function as stopping criteria.
    -   *stepmax*: maximum steps for the training of the neural network,
        if we make it bigger, we let more time for the algorithm to
        converge.

### Test result

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# The outer partitioning was defined in the beginning of the document, we just have to update the preprocess of the data
final_data <- as.data.frame(data) %>% dplyr::select(Creatinine, Ejection.Fraction, Event)
desc_outer <- makeResampleDesc("Holdout", split = 3/4, stratify = TRUE)
final_task <- makeClassifTask(id = "HeartFailure", data = final_data, target = "Event", positive = 1)

set.seed(1234)
partition_outer <- makeResampleInstance(desc_outer, final_task) 


#prueba_learner <- makeLearner("classif.randomForest", mtry = 1, ntree = 266, nodesize = 45)
prueba_learner <- makeLearner("classif.randomForest")
# prueba_learner <- makeLearner("classif.ranger", mtry = 1, num.trees = 45, min.node.size = 25)

set.seed(1234)
prueba_results <- resample(learner = prueba_learner, 
                    task = final_task, 
                    resampling = partition_outer,
                    measures = list(tpr), 
                    models = TRUE,
                    show.info = TRUE)
```

## Survival Analysis

In this section we want to do a comparison of the two different approaches,
statistic-based and generic classification. To do so, we have
independently chosen the most important features given by each approach,
and we will be performing the same machine learning analysis to see
which gets a higher true positive rate. Every parameter (e.g. validation
methodology, search space) will be the same as the previous analysis.

```{r, echo=FALSE, message=FALSE}
data_surv <- as.data.frame(data_surv_tot[train.idxs,]) %>% mutate_at(vars(Gender, Smoking, Diabetes, BP), as.numeric) 
surv_task <- makeClassifTask(id = "HeartFailure", data = data_surv, target = "Event", positive = 1)

set.seed(1234)
desc_inner_surv <- makeResampleInstance("RepCV", reps = 5,  folds = 2, task = surv_task, stratify = TRUE)
```

### Tuning models

```{r, message = FALSE, echo=FALSE, message=FALSE}
# learners, search space and control grid previously defined

parallelStartSocket(5)

set.seed(1234)
tuning_results_surv <- tuneParams(learner = lrn, 
                                  task = surv_task, 
                                  resampling = desc_inner_surv, 
                                  par.set = ps, 
                                  control = control_grid, 
                                  measures = list(tpr),
                                  show.info = FALSE)
parallelStop()
```

### Comparing results
# THIS GRAPH IS TO BE DROPPED
```{r, fig.height=4, fig.width=12, echo=FALSE, message=FALSE}
res_iters_surv <- getTuneResultOptPath(tuning_results_surv)
t_surv <- res_iters_surv %>% dplyr::select(selected.learner, tpr.test.mean)

p_surv <- ggplot(t_surv, aes(tpr.test.mean, selected.learner)) + 
         geom_boxplot(aes(fill = selected.learner))  + 
         labs(x = "True Positive Rate (Surv)", y = "") +scale_fill_brewer(palette="Blues")

ggarrange(p_surv, p_fs, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
```

```{r, echo=FALSE}
fv2 <- generateFilterValuesData(surv_task, method = "FSelectorRcpp_information.gain")
plotFilterValues(fv2, feat.type.cols = TRUE) + ggpubr::theme_pubr() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  ggtitle("Filter Selection \n Information Gain") + scale_fill_brewer(palette="Blues")

fv2$data[order(-value)]
```

```{r, echo=FALSE}
data_surv_fs <- as.data.frame(data_surv) %>% dplyr::select(Event, Creatinine, Ejection.Fraction, Ejection.Fraction_Diabetes, Sodium)
surv_task_fs <- makeClassifTask(id = "HeartFailure", data = data_surv_fs, target = "Event", positive = 1)

set.seed(1234)
desc_inner_surv_fs <- makeResampleInstance("RepCV", reps = 5,  folds = 2, task = surv_task_fs, stratify = TRUE)
```

### Tuning models

```{r, message = FALSE, echo=FALSE}
# learners, search space and control grid previously defined

parallelStartSocket(5)

set.seed(1234)
tuning_results_surv_fs <- tuneParams(learner = lrn, 
                                     task = surv_task_fs, 
                                     resampling = desc_inner_surv_fs, 
                                     par.set = ps, 
                                     control = control_grid, 
                                     measures = list(tpr),
                                     show.info = FALSE)
parallelStop()
```

# THIS SECTION IS WRONG. WE ARE AVERAGING MEANINGLESS THINGS
```{r, fig.height=4, fig.width=12, echo=FALSE}
res_iters_surv_fs <- getTuneResultOptPath(tuning_results_surv_fs)
t_surv_fs <- res_iters_surv_fs %>% dplyr::select(selected.learner, tpr.test.mean)

p_surv_fs <- ggplot(t_surv_fs, aes(tpr.test.mean, selected.learner)) + 
             geom_boxplot(aes(fill = selected.learner))  + 
             labs(x = "True Positive Rate (Surv + FSel)", y = "")  + 
  scale_fill_brewer(palette="Blues")


res1 <- t_surv %>% group_by(selected.learner) %>% summarise(MedianAllFeat=median(tpr.test.mean), MeanAllFeat=mean(tpr.test.mean))
res2 <- t_surv_fs %>% group_by(selected.learner) %>% summarise(MedianFSel=median(tpr.test.mean), MeanFSel=mean(tpr.test.mean))
comparison <- full_join(res1, res2) %>% relocate(selected.learner, MedianAllFeat, MedianFSel)
comparison

ggarrange(p_surv, p_surv_fs, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
```

```{r, echo=FALSE, message=FALSE}
# Obtaining best iteration for each learner (selecting features)
best_res_surv <- res_iters_surv_fs %>% group_by(selected.learner) %>% slice(which.max(tpr.test.mean)) %>% 
                 relocate(tpr.test.mean, .after = selected.learner)

temp2 <- melt(as.data.frame(best_res_surv)) %>% dplyr::select(-error.message) %>% drop_na() 

tpr.test <- temp2 %>% filter(grepl("tpr", variable)) %>% dplyr::rename(tpr = value) %>% dplyr::select(-variable)
hyperparam <- temp2 %>% filter(grepl("classif", variable)) 

tuning_res_surv <- full_join(hyperparam, tpr.test) %>% 
              mutate(parameter = ifelse(grepl("\\.", variable), sub(".*\\.(.*)", "\\1", variable), "")) %>%
              dplyr::select(selected.learner, tpr, parameter, value) 

rm(temp2, tpr.test, hyperparam)
tuning_res_surv
```

### Test result

```{r, echo=FALSE, warning = FALSE, message=FALSE}
final_data_surv <- data_surv_tot %>% dplyr::select(Event, Creatinine, Ejection.Fraction, Ejection.Fraction_Diabetes, Sodium)
final_surv_task <- makeClassifTask(id = "HeartFailure", data = final_data_surv, target = "Event", positive = 1)

set.seed(1234)
partition_outer_surv <- makeResampleInstance(desc_outer, final_surv_task) 

# prueba_learner <- makeLearner("classif.rpart", minsplit = 21, maxdepth = 5)
# prueba_learner <- makeLearner("classif.ranger", mtry = 2, num.trees = 1230, min.node.size = 39)
prueba_learner <- makeLearner("classif.randomForest", mtry = 2, ntree = 943, nodesize = 42)

set.seed(1234)
prueba_results <- resample(learner = prueba_learner, 
                    task = final_surv_task, 
                    resampling = partition_outer_surv,
                    measures = list(tpr), 
                    models = TRUE,
                    show.info = TRUE)
```

#### NOTES

Explicability of the models

Stratification (no need for rebalancing because of is not toooo
unbalanced. Expain why we stratified.)

Algorithms like naive bayes benefit from gaussian data

show.info = FALSE
