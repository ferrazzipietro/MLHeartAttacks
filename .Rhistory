data_classif <- data %>% dplyr::select(Creatinine, Ejection.Fraction, Event)
classif_task <- makeClassifTask(id = "HeartFailure", data = data_classif, target = "Event", positive = 1)
set.seed(1234)
desc_inner <- makeResampleInstance("RepCV", reps = 2,  folds = 4, task = classif_task)
# https://mlr.mlr-org.com/articles/tutorial/integrated_learners.html
classif_measures <- list(tpr, fnr, acc, auc, timetrain)
classif_pipeline <- cpoScaleRange()
## GLM ##
library(h2o)
classif_lrn_glm <- makeLearner("classif.h2o.glm", id = "glm", predict.type = "prob")
classif_lrn_glm <- classif_pipeline %>>% classif_lrn_glm
## Naive Bayes ##
classif_lrn_naive <- makeLearner("classif.naiveBayes", id = "naive", predict.type = "prob")
classif_lrn_naive <- classif_pipeline %>>% classif_lrn_naive
## KNN Model ##
library(kknn)
classif_lrn_knn <- makeLearner("classif.kknn", id = "knn", predict.type = "prob")
classif_lrn_knn <- classif_pipeline %>>% classif_lrn_knn
## LDA ##
library(MASS)
classif_lrn_lda <- makeLearner("classif.lda", id = "LDA", predict.type = "prob")
classif_lrn_lda <- classif_pipeline %>>% classif_lrn_lda
## QDA ##
classif_lrn_qda <- makeLearner("classif.qda", id = "QDA", predict.type = "prob")
classif_lrn_qda <- classif_pipeline %>>% classif_lrn_qda
## Tree(rpart) ##
classif_lrn_rpart <- makeLearner("classif.rpart", id = "rpart", predict.type = "prob")
classif_lrn_rpart <- classif_pipeline %>>% classif_lrn_rpart
## Random Forest ##
classif_lrn_rf <- makeLearner("classif.randomForest", id = "rf", predict.type = "prob")
classif_lrn_rf <- classif_pipeline %>>% classif_lrn_rf
## Ranger ##
classif_lrn_ranger <- makeLearner("classif.ranger", id = "ranger", predict.type = "prob")
classif_lrn_ranger <- classif_pipeline %>>% classif_lrn_ranger
## SVM ##
classif_lrn_SVM <- makeLearner("classif.svm", id = "SVM", predict.type = "prob")
classif_lrn_SVM <- classif_pipeline %>>% classif_lrn_SVM
## Ada Boosting ##
library(ada)
classif_lrn_ada <- makeLearner("classif.ada", id = "ada", predict.type = "prob")
classif_lrn_ada <- classif_pipeline %>>% classif_lrn_ada
## Neural Net ##
# library(neuralnet)
classif_lrn_neuralnet <- makeLearner("classif.neuralnet", id = "neuralnet", predict.type = "prob")
classif_lrn_neuralnet <- classif_pipeline %>>% classif_lrn_neuralnet
## Extreme Gradient Boosting ##
classif_lrn_xgboost <- makeLearner("classif.xgboost", id = "xgboost", predict.type = "prob")
classif_lrn_xgboost <- classif_pipeline %>>% classif_lrn_xgboost
# Repeated cross-validation splits the data in k folds, n reps (folds will be different n times)
# This prevents from noisy estimates of model performance, because different splits may result in very different results. MENTION STRATIFICATION
# outer <- makeResampleDesc("RepCV", reps = 10,
#                           folds = 5, stratify = TRUE)
learners <- list(classif_lrn_glm, classif_lrn_naive, classif_lrn_knn, classif_lrn_lda, classif_lrn_qda,
classif_lrn_rpart, classif_lrn_rf, classif_lrn_ranger, classif_lrn_SVM, classif_lrn_ada,
classif_lrn_neuralnet, classif_lrn_xgboost)
library(parallelMap)
parallelStartSocket(2) # start in socket mode and create 2 processes on localhost
set.seed(1234)
bmr_classif <- benchmark(learners    = learners,
tasks       = classif_task,
resamplings = desc_inner,
measures    = classif_measures,
models      = TRUE,
show.info   = FALSE)
bmr_classif
parallelStop()
bp1 <- plotBMRBoxplots(bmr_classif,
measure   = tpr,
order.lrn = getBMRLearnerIds(bmr_classif))  +
aes(color = learner.id)
bp2 <- plotBMRBoxplots(bmr_classif,
measure   = timetrain,
order.lrn = getBMRLearnerIds(bmr_classif))  +
aes(color = learner.id)
ggarrange(bp1, bp2, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
data_classif <- data %>% dplyr::select(Creatinine, Ejection.Fraction, Event)
classif_task <- makeClassifTask(id = "HeartFailure", data = data_classif, target = "Event", positive = 1)
set.seed(123)
desc_inner <- makeResampleInstance("RepCV", reps = 2,  folds = 4, task = classif_task)
# https://mlr.mlr-org.com/articles/tutorial/integrated_learners.html
classif_measures <- list(tpr, fnr, acc, auc, timetrain)
classif_pipeline <- cpoScaleRange()
## GLM ##
library(h2o)
classif_lrn_glm <- makeLearner("classif.h2o.glm", id = "glm", predict.type = "prob")
classif_lrn_glm <- classif_pipeline %>>% classif_lrn_glm
## Naive Bayes ##
classif_lrn_naive <- makeLearner("classif.naiveBayes", id = "naive", predict.type = "prob")
classif_lrn_naive <- classif_pipeline %>>% classif_lrn_naive
## KNN Model ##
library(kknn)
classif_lrn_knn <- makeLearner("classif.kknn", id = "knn", predict.type = "prob")
classif_lrn_knn <- classif_pipeline %>>% classif_lrn_knn
## LDA ##
library(MASS)
classif_lrn_lda <- makeLearner("classif.lda", id = "LDA", predict.type = "prob")
classif_lrn_lda <- classif_pipeline %>>% classif_lrn_lda
## QDA ##
classif_lrn_qda <- makeLearner("classif.qda", id = "QDA", predict.type = "prob")
classif_lrn_qda <- classif_pipeline %>>% classif_lrn_qda
## Tree(rpart) ##
classif_lrn_rpart <- makeLearner("classif.rpart", id = "rpart", predict.type = "prob")
classif_lrn_rpart <- classif_pipeline %>>% classif_lrn_rpart
## Random Forest ##
classif_lrn_rf <- makeLearner("classif.randomForest", id = "rf", predict.type = "prob")
classif_lrn_rf <- classif_pipeline %>>% classif_lrn_rf
## Ranger ##
classif_lrn_ranger <- makeLearner("classif.ranger", id = "ranger", predict.type = "prob")
classif_lrn_ranger <- classif_pipeline %>>% classif_lrn_ranger
## SVM ##
classif_lrn_SVM <- makeLearner("classif.svm", id = "SVM", predict.type = "prob")
classif_lrn_SVM <- classif_pipeline %>>% classif_lrn_SVM
## Ada Boosting ##
library(ada)
classif_lrn_ada <- makeLearner("classif.ada", id = "ada", predict.type = "prob")
classif_lrn_ada <- classif_pipeline %>>% classif_lrn_ada
## Neural Net ##
# library(neuralnet)
classif_lrn_neuralnet <- makeLearner("classif.neuralnet", id = "neuralnet", predict.type = "prob")
classif_lrn_neuralnet <- classif_pipeline %>>% classif_lrn_neuralnet
## Extreme Gradient Boosting ##
classif_lrn_xgboost <- makeLearner("classif.xgboost", id = "xgboost", predict.type = "prob")
classif_lrn_xgboost <- classif_pipeline %>>% classif_lrn_xgboost
learners <- list(classif_lrn_glm, classif_lrn_naive, classif_lrn_knn, classif_lrn_lda, classif_lrn_qda,
classif_lrn_rpart, classif_lrn_rf, classif_lrn_ranger, classif_lrn_SVM, classif_lrn_ada,
classif_lrn_neuralnet, classif_lrn_xgboost)
library(parallelMap)
parallelStartSocket(2) # start in socket mode and create 2 processes on localhost
set.seed(123)
bmr_classif <- benchmark(learners    = learners,
tasks       = classif_task,
resamplings = desc_inner,
measures    = classif_measures,
models      = TRUE,
show.info   = FALSE)
bmr_classif
parallelStop()
bp1 <- plotBMRBoxplots(bmr_classif,
measure   = tpr,
order.lrn = getBMRLearnerIds(bmr_classif))  +
aes(color = learner.id)
bp2 <- plotBMRBoxplots(bmr_classif,
measure   = timetrain,
order.lrn = getBMRLearnerIds(bmr_classif))  +
aes(color = learner.id)
ggarrange(bp1, bp2, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
configureMlr(show.info = TRUE, on.learner.warning = "warn") # see all info
# configureMlr(show.info = FALSE, on.learner.warning = "quiet") # so we don't get too many messages
bls <- list(makeLearner("classif.kknn"),
makeLearner("classif.rpart"),
makeLearner("classif.randomForest"),
makeLearner("classif.ranger"),
makeLearner("classif.svm"),
makeLearner("classif.ada"),
makeLearner("classif.xgboost"),
makeLearner("classif.neuralnet"))
lrn <- makeModelMultiplexer(bls)
# Parameter Space (all together, divided by learner inside)
ps <- makeModelMultiplexerParamSet(
multiplexer = lrn,
classif.kknn = makeParamSet(makeIntegerParam("k", lower = 5, upper = 15)), # Default is 7-
classif.rpart = makeParamSet(makeIntegerParam("minsplit", lower = 15, upper = 35), # Default is 20
makeIntegerParam("maxdepth", lower = 5, upper = 15)), # Default is 30
classif.randomForest = makeParamSet(makeIntegerParam("ntree", lower = 100, upper = 1500), #Default is 500
makeIntegerParam("mtry",lower = 1, upper = 2), #Default is floor(sqrt(features/3)), cannot be larger than number of variables
makeIntegerParam("nodesize", lower = 10, upper = 50)), #Default is 1
classif.ranger = makeParamSet(makeIntegerParam("mtry", lower = 1, upper = 2), # Same parameters as randomForest
makeIntegerParam("num.trees", lower = 100, upper = 1500),
makeIntegerParam("min.node.size", lower = 10, upper = 50)),
classif.svm = makeParamSet(makeNumericParam("cost", lower = 0.1, upper = 10), # Default is 1
makeNumericParam("gamma", lower = 0.05, upper = 5)), # Default is (1/data dimension)
classif.ada = makeParamSet(makeNumericParam("nu", lower = 0.05, upper = 3), # Default 0.1
makeIntegerParam("iter", lower = 20, upper = 200)), # Default is 50
classif.xgboost = makeParamSet(makeNumericParam("eta", lower = 0.2, upper = 0.5), # Default is 0.3
makeIntegerParam("nrounds", lower = 4, upper = 10), # Default is
makeIntegerParam("max_depth", lower = 3, upper = 10)), # Default is 6
classif.neuralnet = makeParamSet(makeIntegerVectorParam("hidden", len = 1, lower = 5, upper = 10),
makeNumericParam("threshold", lower = 2, upper = 5, trafo = function(x) {10^x}),
makeNumericParam("stepmax", lower = 2, upper = 2, trafo = function(x) {10^x}))
)
# Validation
desc_tuning <- makeResampleDesc("CV", iters = 4)
control_grid <- makeTuneControlRandom(maxit = 200L) # Default is 100, split between the amount of learners
set.seed(123)
tuning <- makeTuneWrapper(learner = lrn,
resampling = desc_tuning,
par.set = ps,
control = control_grid,
measures = list(tpr),
show.info = TRUE)
# Parallelize model evaluation (it takes pretty long)
#parallelStartSocket(5) # start in socket mode and create 2 processes on localhost
set.seed(123)
results <- mlr::resample(learner = tuning,
task = classif_task,
resampling = desc_inner,
measures = list(tpr),
models = TRUE,
extract = getTuneResult,
show.info = TRUE)
parallelStop()
res_iters <- getNestedTuneResultsOptPathDf(results)
?xgboost
bls <- list(makeLearner("classif.kknn"),
makeLearner("classif.rpart"),
makeLearner("classif.randomForest"),
makeLearner("classif.ranger"),
makeLearner("classif.svm"),
makeLearner("classif.ada"),
makeLearner("classif.xgboost", eval_metric = "aucpr"),
makeLearner("classif.neuralnet"))
lrn <- makeModelMultiplexer(bls)
# Parameter Space (all together, divided by learner inside)
ps <- makeModelMultiplexerParamSet(
multiplexer = lrn,
classif.kknn = makeParamSet(makeIntegerParam("k", lower = 5, upper = 15)), # Default is 7-
classif.rpart = makeParamSet(makeIntegerParam("minsplit", lower = 15, upper = 35), # Default is 20
makeIntegerParam("maxdepth", lower = 5, upper = 15)), # Default is 30
classif.randomForest = makeParamSet(makeIntegerParam("ntree", lower = 100, upper = 1500), #Default is 500
makeIntegerParam("mtry",lower = 1, upper = 2), #Default is floor(sqrt(features/3)), cannot be larger than number of variables
makeIntegerParam("nodesize", lower = 10, upper = 50)), #Default is 1
classif.ranger = makeParamSet(makeIntegerParam("mtry", lower = 1, upper = 2), # Same parameters as randomForest
makeIntegerParam("num.trees", lower = 100, upper = 1500),
makeIntegerParam("min.node.size", lower = 10, upper = 50)),
classif.svm = makeParamSet(makeNumericParam("cost", lower = 0.1, upper = 10), # Default is 1
makeNumericParam("gamma", lower = 0.05, upper = 5)), # Default is (1/data dimension)
classif.ada = makeParamSet(makeNumericParam("nu", lower = 0.05, upper = 3), # Default 0.1
makeIntegerParam("iter", lower = 20, upper = 200)), # Default is 50
classif.xgboost = makeParamSet(makeNumericParam("eta", lower = 0.2, upper = 0.5), # Default is 0.3
makeIntegerParam("nrounds", lower = 4, upper = 10), # Default is
makeIntegerParam("max_depth", lower = 3, upper = 10)), # Default is 6
classif.neuralnet = makeParamSet(makeIntegerVectorParam("hidden", len = 1, lower = 5, upper = 10),
makeNumericParam("threshold", lower = 2, upper = 5, trafo = function(x) {10^x}),
makeNumericParam("stepmax", lower = 2, upper = 2, trafo = function(x) {10^x}))
)
# Validation
desc_tuning <- makeResampleDesc("CV", iters = 4)
control_grid <- makeTuneControlRandom(maxit = 200L) # Default is 100, split between the amount of learners
set.seed(123)
tuning <- makeTuneWrapper(learner = lrn,
resampling = desc_tuning,
par.set = ps,
control = control_grid,
measures = list(tpr),
show.info = TRUE)
# Parallelize model evaluation (it takes pretty long)
#parallelStartSocket(5) # start in socket mode and create 2 processes on localhost
set.seed(123)
results <- mlr::resample(learner = tuning,
task = classif_task,
resampling = desc_inner,
measures = list(tpr),
models = TRUE,
extract = getTuneResult,
show.info = TRUE)
parallelStop()
res_iters <- getNestedTuneResultsOptPathDf(results)
parallelStop()
# Parallelize model evaluation (it takes pretty long)
parallelStartSocket(5) # start in socket mode and create 2 processes on localhost
set.seed(123)
results <- mlr::resample(learner = tuning,
task = classif_task,
resampling = desc_inner,
measures = list(tpr),
models = TRUE,
extract = getTuneResult,
show.info = TRUE)
bls <- list(makeLearner("classif.kknn"),
makeLearner("classif.rpart"),
makeLearner("classif.randomForest"),
makeLearner("classif.ranger"),
makeLearner("classif.svm"),
makeLearner("classif.ada"),
makeLearner("classif.xgboost", eval_metric = "aucpr"),
makeLearner("classif.neuralnet"))
lrn <- makeModelMultiplexer(bls)
# Parameter Space (all together, divided by learner inside)
ps <- makeModelMultiplexerParamSet(
multiplexer = lrn,
classif.kknn = makeParamSet(makeIntegerParam("k", lower = 5, upper = 15)), # Default is 7-
classif.rpart = makeParamSet(makeIntegerParam("minsplit", lower = 15, upper = 35), # Default is 20
makeIntegerParam("maxdepth", lower = 5, upper = 15)), # Default is 30
classif.randomForest = makeParamSet(makeIntegerParam("ntree", lower = 100, upper = 1500), #Default is 500
makeIntegerParam("mtry",lower = 1, upper = 2), #Default is floor(sqrt(features/3)), cannot be larger than number of variables
makeIntegerParam("nodesize", lower = 10, upper = 50)), #Default is 1
classif.ranger = makeParamSet(makeIntegerParam("mtry", lower = 1, upper = 2), # Same parameters as randomForest
makeIntegerParam("num.trees", lower = 100, upper = 1500),
makeIntegerParam("min.node.size", lower = 10, upper = 50)),
classif.svm = makeParamSet(makeNumericParam("cost", lower = 0.1, upper = 10), # Default is 1
makeNumericParam("gamma", lower = 0.05, upper = 5)), # Default is (1/data dimension)
classif.ada = makeParamSet(makeNumericParam("nu", lower = 0.05, upper = 3), # Default 0.1
makeIntegerParam("iter", lower = 20, upper = 200)), # Default is 50
classif.xgboost = makeParamSet(makeNumericParam("eta", lower = 0.2, upper = 0.5), # Default is 0.3
makeIntegerParam("nrounds", lower = 4, upper = 10), # Default is
makeIntegerParam("max_depth", lower = 3, upper = 10)), # Default is 6
classif.neuralnet = makeParamSet(makeIntegerVectorParam("hidden", len = 1, lower = 5, upper = 10),
makeNumericParam("threshold", lower = 2, upper = 5, trafo = function(x) {10^x}),
makeNumericParam("stepmax", lower = 2, upper = 2, trafo = function(x) {10^x}))
)
# Validation
desc_tuning <- makeResampleDesc("CV", iters = 4)
control_grid <- makeTuneControlRandom(maxit = 200L) # Default is 100, split between the amount of learners
set.seed(123)
tuning <- makeTuneWrapper(learner = lrn,
resampling = desc_tuning,
par.set = ps,
control = control_grid,
measures = list(tpr),
show.info = TRUE)
parallelStop()
# Parallelize model evaluation (it takes pretty long)
#parallelStartSocket(5) # start in socket mode and create 2 processes on localhost
set.seed(123)
results <- mlr::resample(learner = tuning,
task = classif_task,
resampling = desc_inner,
measures = list(tpr),
models = TRUE,
extract = getTuneResult,
show.info = TRUE)
parallelStop()
res_iters <- getNestedTuneResultsOptPathDf(results)
best_res <- res_iters %>% group_by(selected.learner) %>% slice(which.max(tpr.test.mean)) %>% relocate(tpr.test.mean, .after = selected.learner)
best_res
best_res
g <- ggplot(res_iters, aes(y=tpr.test.mean))
bp3 <- g + geom_boxplot(aes(x=selected.learner))
bp4 <- plotBMRBoxplots(bmr_classif,
measure   = timetrain,
order.lrn = getBMRLearnerIds(bmr_classif))  +
aes(color = learner.id)
ggarrange(bp1, bp3, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
temp1 <- best_res %>% dplyr::select(selected.learner, tpr.test.mean)
temp1
ggplot(temp1, aes(x=selected.learner, y=tpr.test.mean, color=tpr.test.mean, fill = tpr.test.mean)) +
geom_segment(aes(x=selected.learner, xend=selected.learner, y=0, yend=tpr.test.mean)) +
geom_point(size=3, alpha=0.7, shape=21, stroke=2) +
coord_flip()
bp3 <- plotBMRBoxplots(bmr_classif,
measure   = tpr,
order.lrn = getBMRLearnerIds(bmr_classif))  +
aes(color = learner.id)
bp4 <- plotBMRBoxplots(bmr_classif,
measure   = timetrain,
order.lrn = getBMRLearnerIds(bmr_classif))  +
aes(color = learner.id)
ggarrange(bp3, bp4, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
View(res_iters)
g <- ggplot(res_iters, aes(y=tpr.test.mean, color = selected.learner))
bp3 <- g + geom_boxplot(aes(x=selected.learner))
bp4 <- plotBMRBoxplots(bmr_classif,
measure   = timetrain,
order.lrn = getBMRLearnerIds(bmr_classif))  +
aes(color = learner.id)
ggarrange(bp3, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
bp3 <- ggplot(res_iters, aes(y=tpr.test.mean, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner))
bp4 <- ggplot(res_iters, aes(y=exec.time, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner))
ggarrange(bp3, bp4, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
?makeResampleInstance
data_classif <- data %>% dplyr::select(Creatinine, Ejection.Fraction, Event)
classif_task <- makeClassifTask(id = "HeartFailure", data = data_classif, target = "Event", positive = 1)
set.seed(123)
desc_inner <- makeResampleInstance("RepCV", reps = 2,  folds = 4, task = classif_task, stratify = TRUE)
# https://mlr.mlr-org.com/articles/tutorial/integrated_learners.html
classif_measures <- list(tpr, fnr, acc, auc, timetrain)
classif_pipeline <- cpoScaleRange()
## GLM ##
library(h2o)
classif_lrn_glm <- makeLearner("classif.h2o.glm", id = "glm", predict.type = "prob")
classif_lrn_glm <- classif_pipeline %>>% classif_lrn_glm
## Naive Bayes ##
classif_lrn_naive <- makeLearner("classif.naiveBayes", id = "naive", predict.type = "prob")
classif_lrn_naive <- classif_pipeline %>>% classif_lrn_naive
## KNN Model ##
library(kknn)
classif_lrn_knn <- makeLearner("classif.kknn", id = "knn", predict.type = "prob")
classif_lrn_knn <- classif_pipeline %>>% classif_lrn_knn
## LDA ##
library(MASS)
classif_lrn_lda <- makeLearner("classif.lda", id = "LDA", predict.type = "prob")
classif_lrn_lda <- classif_pipeline %>>% classif_lrn_lda
## QDA ##
classif_lrn_qda <- makeLearner("classif.qda", id = "QDA", predict.type = "prob")
classif_lrn_qda <- classif_pipeline %>>% classif_lrn_qda
## Tree(rpart) ##
classif_lrn_rpart <- makeLearner("classif.rpart", id = "rpart", predict.type = "prob")
classif_lrn_rpart <- classif_pipeline %>>% classif_lrn_rpart
## Random Forest ##
classif_lrn_rf <- makeLearner("classif.randomForest", id = "rf", predict.type = "prob")
classif_lrn_rf <- classif_pipeline %>>% classif_lrn_rf
## Ranger ##
classif_lrn_ranger <- makeLearner("classif.ranger", id = "ranger", predict.type = "prob")
classif_lrn_ranger <- classif_pipeline %>>% classif_lrn_ranger
## SVM ##
classif_lrn_SVM <- makeLearner("classif.svm", id = "SVM", predict.type = "prob")
classif_lrn_SVM <- classif_pipeline %>>% classif_lrn_SVM
## Ada Boosting ##
library(ada)
classif_lrn_ada <- makeLearner("classif.ada", id = "ada", predict.type = "prob")
classif_lrn_ada <- classif_pipeline %>>% classif_lrn_ada
## Neural Net ##
# library(neuralnet)
classif_lrn_neuralnet <- makeLearner("classif.neuralnet", id = "neuralnet", predict.type = "prob")
classif_lrn_neuralnet <- classif_pipeline %>>% classif_lrn_neuralnet
## Extreme Gradient Boosting ##
classif_lrn_xgboost <- makeLearner("classif.xgboost", id = "xgboost", predict.type = "prob")
classif_lrn_xgboost <- classif_pipeline %>>% classif_lrn_xgboost
learners <- list(classif_lrn_glm, classif_lrn_naive, classif_lrn_knn, classif_lrn_lda, classif_lrn_qda,
classif_lrn_rpart, classif_lrn_rf, classif_lrn_ranger, classif_lrn_SVM, classif_lrn_ada,
classif_lrn_neuralnet, classif_lrn_xgboost)
library(parallelMap)
parallelStartSocket(2) # start in socket mode and create 2 processes on localhost
set.seed(123)
bmr_classif <- benchmark(learners    = learners,
tasks       = classif_task,
resamplings = desc_inner,
measures    = classif_measures,
models      = TRUE,
show.info   = FALSE)
bmr_classif
parallelStop()
bp3 <- ggplot(res_iters, aes(y=tpr.test.mean, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner))
bp4 <- ggplot(res_iters, aes(y=exec.time, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner))
ggarrange(bp3, bp4, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
bp3 <- ggplot(res_iters, aes(y=tpr.test.mean, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
bp4 <- ggplot(res_iters, aes(y=exec.time, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner))
ggarrange(bp3, bp4, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
bp3 <- ggplot(res_iters, aes(y=tpr.test.mean, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 50, vjust = 0.5, hjust=1))
bp4 <- ggplot(res_iters, aes(y=exec.time, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 50, vjust = 0.5, hjust=1))
ggarrange(bp3, bp4, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
bp3 <- ggplot(res_iters, aes(y=tpr.test.mean, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 50, hjust=1))
bp4 <- ggplot(res_iters, aes(y=exec.time, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 50, hjust=1))
ggarrange(bp3, bp4, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
bp3 <- ggplot(res_iters, aes(y=tpr.test.mean, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 25, hjust=1))
bp4 <- ggplot(res_iters, aes(y=exec.time, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 25, hjust=1))
ggarrange(bp3, bp4, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
bp3 <- ggplot(res_iters, aes(y=tpr.test.mean, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 25, hjust=1))
bp4 <- ggplot(res_iters, aes(y=exec.time, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 25, hjust=1), axis.title = FALSE, axis.title.y = "True Positive Rate")
ggarrange(bp3, bp4, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
bp3 <- ggplot(res_iters, aes(y=tpr.test.mean, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 25, hjust=1))
bp4 <- ggplot(res_iters, aes(y=exec.time, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 25, hjust=1), axis.title.x = "", axis.title.y = "True Positive Rate")
ggarrange(bp3, bp4, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
bp3 <- ggplot(res_iters, aes(y=tpr.test.mean, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 25, hjust=1))
bp4 <- ggplot(res_iters, aes(y=exec.time, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 25, hjust=1), axis.title.x = element_blank(), axis.title.y = "True Positive Rate")
ggarrange(bp3, bp4, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
bp3 <- ggplot(res_iters, aes(y=tpr.test.mean, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 25, hjust=1))
bp4 <- ggplot(res_iters, aes(y=exec.time, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 25, hjust=1), axis.title.x = element_blank(), axis.title.y = element_text(True Positive Rate))
bp3 <- ggplot(res_iters, aes(y=tpr.test.mean, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 25, hjust=1))
bp4 <- ggplot(res_iters, aes(y=exec.time, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 25, hjust=1), axis.title.x = element_blank(), axis.title.y = element_text("True Positive Rate"))
ggarrange(bp3, bp4, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
bp3 <- ggplot(res_iters, aes(y=tpr.test.mean, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 25, hjust=1))
bp4 <- ggplot(res_iters, aes(y=exec.time, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 25, hjust=1)) +
labs(x = "", y = "True Positive Rate")
ggarrange(bp3, bp4, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
bp3 <- ggplot(res_iters, aes(y=tpr.test.mean, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 25, hjust=1)) +
labs(x = "", y = "True Positive Rate")
bp4 <- ggplot(res_iters, aes(y=exec.time, color = selected.learner)) +
geom_boxplot(aes(x=selected.learner)) +
theme(axis.text.x = element_text(angle = 25, hjust=1)) +
labs(x = "", y = "Execution Time")
ggarrange(bp3, bp4, ncol = 2, nrow = 1, common.legend = TRUE, legend = "right")
View(best_res)
library(rpart)
library(rpart.plot)
install.packages("rpart.plot")
detach(rpart.plot)
